{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Submitted_VAE_31_ Dec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTTFeMThLina",
        "outputId": "8582d9cd-c9ae-4ac4-be46-c449215926d9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')scr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GYx3DOHLscu"
      },
      "source": [
        "import os\r\n",
        "os.chdir(\"/content/gdrive/My Drive/dataset)\r\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5njaGx07L0JE"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "from torch import nn\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import transforms\r\n",
        "import torchvision.datasets as dset\r\n",
        "from torchvision.utils import save_image\r\n",
        "import torchvision.utils as vutils\r\n",
        "from torchsummary import summary\r\n",
        "from IPython import display\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zngPnmDH-je5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhmW6yBUISF"
      },
      "source": [
        "Parameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dcdk29k-ebg"
      },
      "source": [
        "\n",
        "latent_dims = 2\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "variational_beta = 1\n",
        "use_gpu = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPUZ1OAfDGn8"
      },
      "source": [
        "dataroot = \"./\"\n",
        "image_size = 128\n",
        "nb_channls=3\n",
        "workers=4\n",
        "test_batch_size = 2\n",
        "weight_decay = 1e-5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pISpfCp0T_UI"
      },
      "source": [
        "Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1nHma3x-pdb",
        "outputId": "bf6ebeff-60bc-4902-e8c6-54037439b356"
      },
      "source": [
        "# Load Dataset\n",
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize((image_size, image_size)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "train_splitRatio = 0.8\n",
        "train_len = int(train_splitRatio * len(dataset))\n",
        "test_len = len(dataset) - train_len\n",
        "\n",
        "print(train_len)\n",
        "trainData, testData = torch.utils.data.random_split(dataset,[train_len, test_len])\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(trainData, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers,drop_last=True)\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(testData, batch_size=test_batch_size,\n",
        "                                         shuffle=False, num_workers=workers,drop_last=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL0Chi_FTxX6"
      },
      "source": [
        "VAE Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Q0lB0C-r19",
        "outputId": "921c8d29-a988-4209-9c26-6cab5208608f"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=c, kernel_size=4, stride=2, padding=1) \n",
        "        #self.conv1_bn = nn.BatchNorm2d(c)\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n",
        "        #self.conv2_bn = nn.BatchNorm2d(c*2)\n",
        "        self.fc_mu = nn.Linear(in_features=c*2*32*32, out_features=latent_dims)\n",
        "        #self.fc_mu_bn = nn.BatchNorm1d(latent_dims)\n",
        "        self.fc_logvar = nn.Linear(in_features=c*2*32*32, out_features=latent_dims)\n",
        "        #self.fc_logvar_bn = nn.BatchNorm1d(latent_dims)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = F.relu((self.conv1(x)))\n",
        "        x = F.relu((self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x_mu = (self.fc_mu(x))\n",
        "        x_logvar = (self.fc_logvar(x))\n",
        "        return x_mu, x_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*32*32)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        #self.conv2_bn = nn.BatchNorm2d(c)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
        "        #self.conv1_bn = nn.BatchNorm2d(3)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*2, 32, 32) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu((self.conv2(x)))\n",
        "        x = torch.sigmoid((self.conv1(x))) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "        return x\n",
        "    \n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "    \n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "   \n",
        "    mse_loss = nn.MSELoss(reduction=\"sum\")\n",
        "    loss_MSE = mse_loss(recon_x, x)\n",
        "    \n",
        "    # KL-divergence between the prior distribution over latent vectors\n",
        "    # (the one we are going to sample from when generating new images)\n",
        "    # and the distribution estimated by the generator for the given image.\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    kldivergence /= batch_size\n",
        "    \n",
        "    return loss_MSE + variational_beta * kldivergence\n",
        "    \n",
        "    \n",
        "vae = VariationalAutoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "vae = vae.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 1186055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTkhOJHgUVPM",
        "outputId": "ae05b8db-9483-4d27-a629-06ba8c9d2354"
      },
      "source": [
        "summary(vae, (nb_channls, image_size, image_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           3,136\n",
            "            Conv2d-2          [-1, 128, 32, 32]         131,200\n",
            "            Linear-3                    [-1, 2]         262,146\n",
            "            Linear-4                    [-1, 2]         262,146\n",
            "           Encoder-5         [[-1, 2], [-1, 2]]               0\n",
            "            Linear-6               [-1, 131072]         393,216\n",
            "   ConvTranspose2d-7           [-1, 64, 64, 64]         131,136\n",
            "   ConvTranspose2d-8          [-1, 3, 128, 128]           3,075\n",
            "           Decoder-9          [-1, 3, 128, 128]               0\n",
            "================================================================\n",
            "Total params: 1,186,055\n",
            "Trainable params: 1,186,055\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 6.75\n",
            "Params size (MB): 4.52\n",
            "Estimated Total Size (MB): 11.46\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ZUHnlkT6Tu"
      },
      "source": [
        "Train VAE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6x8kNjI-ttm",
        "outputId": "053085db-ab7c-4b19-82ec-57e8ec3d454a"
      },
      "source": [
        "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# set to training mode\n",
        "vae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch, _ in dataloader_train:\n",
        "        \n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "        \n",
        "        # reconstruction error\n",
        "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "Epoch [1 / 100] average reconstruction error: 3316156.500000\n",
            "Epoch [2 / 100] average reconstruction error: 3227932.886905\n",
            "Epoch [3 / 100] average reconstruction error: 3143409.672619\n",
            "Epoch [4 / 100] average reconstruction error: 3067079.880952\n",
            "Epoch [5 / 100] average reconstruction error: 2998941.196429\n",
            "Epoch [6 / 100] average reconstruction error: 2932161.767857\n",
            "Epoch [7 / 100] average reconstruction error: 2868755.434524\n",
            "Epoch [8 / 100] average reconstruction error: 2810749.369048\n",
            "Epoch [9 / 100] average reconstruction error: 2753740.589286\n",
            "Epoch [10 / 100] average reconstruction error: 2700309.869048\n",
            "Epoch [11 / 100] average reconstruction error: 2648283.523810\n",
            "Epoch [12 / 100] average reconstruction error: 2599499.571429\n",
            "Epoch [13 / 100] average reconstruction error: 2555055.315476\n",
            "Epoch [14 / 100] average reconstruction error: 2515107.952381\n",
            "Epoch [15 / 100] average reconstruction error: 2472539.434524\n",
            "Epoch [16 / 100] average reconstruction error: 2435179.488095\n",
            "Epoch [17 / 100] average reconstruction error: 2400187.172619\n",
            "Epoch [18 / 100] average reconstruction error: 2366573.827381\n",
            "Epoch [19 / 100] average reconstruction error: 2334505.726190\n",
            "Epoch [20 / 100] average reconstruction error: 2305775.395833\n",
            "Epoch [21 / 100] average reconstruction error: 2276053.714286\n",
            "Epoch [22 / 100] average reconstruction error: 2245346.538690\n",
            "Epoch [23 / 100] average reconstruction error: 2222688.678571\n",
            "Epoch [24 / 100] average reconstruction error: 2200085.318452\n",
            "Epoch [25 / 100] average reconstruction error: 2178810.627976\n",
            "Epoch [26 / 100] average reconstruction error: 2153915.276786\n",
            "Epoch [27 / 100] average reconstruction error: 2130623.687500\n",
            "Epoch [28 / 100] average reconstruction error: 2112180.107143\n",
            "Epoch [29 / 100] average reconstruction error: 2093453.342262\n",
            "Epoch [30 / 100] average reconstruction error: 2074957.449405\n",
            "Epoch [31 / 100] average reconstruction error: 2057202.199405\n",
            "Epoch [32 / 100] average reconstruction error: 2043917.449405\n",
            "Epoch [33 / 100] average reconstruction error: 2028288.497024\n",
            "Epoch [34 / 100] average reconstruction error: 2010906.854167\n",
            "Epoch [35 / 100] average reconstruction error: 1997585.285714\n",
            "Epoch [36 / 100] average reconstruction error: 1985523.806548\n",
            "Epoch [37 / 100] average reconstruction error: 1972392.553571\n",
            "Epoch [38 / 100] average reconstruction error: 1958822.250000\n",
            "Epoch [39 / 100] average reconstruction error: 1944701.550595\n",
            "Epoch [40 / 100] average reconstruction error: 1933769.336310\n",
            "Epoch [41 / 100] average reconstruction error: 1924501.973214\n",
            "Epoch [42 / 100] average reconstruction error: 1910260.776786\n",
            "Epoch [43 / 100] average reconstruction error: 1903968.395833\n",
            "Epoch [44 / 100] average reconstruction error: 1894023.940476\n",
            "Epoch [45 / 100] average reconstruction error: 1883881.267857\n",
            "Epoch [46 / 100] average reconstruction error: 1873005.696429\n",
            "Epoch [47 / 100] average reconstruction error: 1862947.360119\n",
            "Epoch [48 / 100] average reconstruction error: 1856778.095238\n",
            "Epoch [49 / 100] average reconstruction error: 1848357.422619\n",
            "Epoch [50 / 100] average reconstruction error: 1842851.285714\n",
            "Epoch [51 / 100] average reconstruction error: 1834008.294643\n",
            "Epoch [52 / 100] average reconstruction error: 1828731.401786\n",
            "Epoch [53 / 100] average reconstruction error: 1819221.681548\n",
            "Epoch [54 / 100] average reconstruction error: 1815215.755952\n",
            "Epoch [55 / 100] average reconstruction error: 1805648.875000\n",
            "Epoch [56 / 100] average reconstruction error: 1801303.747024\n",
            "Epoch [57 / 100] average reconstruction error: 1795064.925595\n",
            "Epoch [58 / 100] average reconstruction error: 1788486.348214\n",
            "Epoch [59 / 100] average reconstruction error: 1782916.505952\n",
            "Epoch [60 / 100] average reconstruction error: 1778373.910714\n",
            "Epoch [61 / 100] average reconstruction error: 1774093.386905\n",
            "Epoch [62 / 100] average reconstruction error: 1769840.291667\n",
            "Epoch [63 / 100] average reconstruction error: 1763958.333333\n",
            "Epoch [64 / 100] average reconstruction error: 1756606.723214\n",
            "Epoch [65 / 100] average reconstruction error: 1753451.312500\n",
            "Epoch [66 / 100] average reconstruction error: 1748391.610119\n",
            "Epoch [67 / 100] average reconstruction error: 1743722.669643\n",
            "Epoch [68 / 100] average reconstruction error: 1740231.392857\n",
            "Epoch [69 / 100] average reconstruction error: 1737384.110119\n",
            "Epoch [70 / 100] average reconstruction error: 1731735.386905\n",
            "Epoch [71 / 100] average reconstruction error: 1729335.705357\n",
            "Epoch [72 / 100] average reconstruction error: 1725408.985119\n",
            "Epoch [73 / 100] average reconstruction error: 1718715.214286\n",
            "Epoch [74 / 100] average reconstruction error: 1717608.714286\n",
            "Epoch [75 / 100] average reconstruction error: 1717213.857143\n",
            "Epoch [76 / 100] average reconstruction error: 1715151.601190\n",
            "Epoch [77 / 100] average reconstruction error: 1706911.401786\n",
            "Epoch [78 / 100] average reconstruction error: 1705200.866071\n",
            "Epoch [79 / 100] average reconstruction error: 1699782.613095\n",
            "Epoch [80 / 100] average reconstruction error: 1700129.425595\n",
            "Epoch [81 / 100] average reconstruction error: 1696422.830357\n",
            "Epoch [82 / 100] average reconstruction error: 1690684.976190\n",
            "Epoch [83 / 100] average reconstruction error: 1690616.571429\n",
            "Epoch [84 / 100] average reconstruction error: 1687876.258929\n",
            "Epoch [85 / 100] average reconstruction error: 1685409.196429\n",
            "Epoch [86 / 100] average reconstruction error: 1680302.434524\n",
            "Epoch [87 / 100] average reconstruction error: 1677538.657738\n",
            "Epoch [88 / 100] average reconstruction error: 1679116.479167\n",
            "Epoch [89 / 100] average reconstruction error: 1676003.744048\n",
            "Epoch [90 / 100] average reconstruction error: 1672382.261905\n",
            "Epoch [91 / 100] average reconstruction error: 1674377.482143\n",
            "Epoch [92 / 100] average reconstruction error: 1668992.916667\n",
            "Epoch [93 / 100] average reconstruction error: 1667775.693452\n",
            "Epoch [94 / 100] average reconstruction error: 1663397.166667\n",
            "Epoch [95 / 100] average reconstruction error: 1665841.559524\n",
            "Epoch [96 / 100] average reconstruction error: 1659450.101190\n",
            "Epoch [97 / 100] average reconstruction error: 1659256.315476\n",
            "Epoch [98 / 100] average reconstruction error: 1655824.139881\n",
            "Epoch [99 / 100] average reconstruction error: 1655431.648810\n",
            "Epoch [100 / 100] average reconstruction error: 1654672.654762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpQbOCerSIrh"
      },
      "source": [
        "Plot Training Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "oUZ3at3YRkz7",
        "outputId": "6137e297-0424-40cc-dd01-3a0b4ef3e67f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwWEHjJ4SFmY"
      },
      "source": [
        "Evaluate on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w32Np7L_Rq1o",
        "outputId": "94f3e06a-7368-40e3-ff81-6246eeddff88"
      },
      "source": [
        "# set to evaluation mode\n",
        "vae.eval()\n",
        "\n",
        "test_loss_avg, num_batches = 0, 0\n",
        "for image_batch, _ in dataloader_test:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "\n",
        "        test_loss_avg += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "test_loss_avg /= num_batches\n",
        "print('average reconstruction error: %f' % (test_loss_avg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcc75330ba8>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcc75330ba8>>\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "Traceback (most recent call last):\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcbdab48b38>>\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "Traceback (most recent call last):\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcbdab48b38>>\n",
            "AssertionError: can only join a child process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcc75330ba8>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcc75330ba8>>\n",
            "AssertionError: can only join a child process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcbdab48b38>>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "Traceback (most recent call last):\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fcbdab48b38>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average reconstruction error: 25514.449980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX61fsE3dBdF",
        "outputId": "efabfd94-1af7-4573-d76e-9a41f7aab473"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vdQkC8OSCJo"
      },
      "source": [
        "Visualize Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIq7g5bZR7y9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        images = images.to(device)\n",
        "        images, _, _ = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = iter(dataloader_test).next()\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the vae\n",
        "print('VAE reconstruction:')\n",
        "visualise_output(images, vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5r9alXSE-UD"
      },
      "source": [
        "#%debug\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}